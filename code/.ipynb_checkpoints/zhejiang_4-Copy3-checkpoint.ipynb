{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 此版本将测试集中出现的情况删除，以及去除了故障月份和使用时长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 浙江省第四类故障预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.sparse import coo_matrix, bmat\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ./data/all_4.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8e14b789e8c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                   \u001b[0;34m'FAULT_TYPE'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SYNC_ORG_NO'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ORG_NO'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                  }, encoding='utf-8')\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#preview the zhejiang_4 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4025)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8031)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ./data/all_4.csv does not exist"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/usr/local/hadoop/src/code/dianbiao/data/all_4.csv', dtype={0:object, 'ORG':object, 'SPEC_CODE':object, 'MANUFACTURER':object,\n",
    "                                                  'FAULT_MONTH':int, 'INST_MONTH':object, 'FAULT_QUARTER':object, \n",
    "                                                  'FAULT_TYPE': object,'SYNC_ORG_NO':object,'ORG_NO':object\n",
    "                                              \n",
    "                                                 }, encoding='utf-8')\n",
    "\n",
    "#preview the zhejiang_4 data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#delete QUIP_ID\n",
    "#data.drop([data.columns[0]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.drop([data.columns[0]], axis=1, inplace=True)\n",
    "#data.drop_duplicates(['FAULT_TYPE', 'ORG_NO', 'SPEC_CODE', 'COMM_MODE', 'MANUFACTURER', 'FAULT_MONTH',\n",
    " #                    'INST_MONTH', 'month'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.info()\n",
    "data['FAULT_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axis0 = plt.subplots(1, 1)\n",
    "sns.countplot(x='FAULT_TYPE', data=data, ax=axis0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从故障类型柱状图可以看出故障类型数据不平衡，402-406较少，407-411较多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SYNC_ORG_NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ORG\n",
    "print data['SYNC_ORG_NO'].describe()\n",
    "#plot\n",
    "def plot_fun(name_fea, name_fault, fontsize=None):\n",
    "    \n",
    "    fig, axis1 = plt.subplots(1, 1)\n",
    "    sns.countplot(x=name_fea, data=data, ax = axis1)\n",
    "\n",
    "    fig, axis2 = plt.subplots(1, 1)\n",
    "    c = data[name_fea].value_counts()\n",
    "    s = c.cumsum()/c.sum()\n",
    "    axis2.plot(np.arange(s.shape[0])+1, s.values*100)\n",
    "    axis2.set_title('precent of %s'%name_fea)\n",
    "\n",
    "    fig, axis3 = plt.subplots(1, 1)\n",
    "    sns.countplot(x=name_fea, hue=name_fault, data=data, ax=axis3)\n",
    "    plt.legend(loc = 2)\n",
    "\n",
    "    fig, axis4 = plt.subplots(1, 1)\n",
    "    sns.countplot(x=name_fault, hue=name_fea, data=data, ax=axis4)\n",
    "    plt.legend(loc = 2, fontsize=fontsize)\n",
    "\n",
    "    #calculate similar score\n",
    "    from scipy.cluster.hierarchy import dendrogram, linkage    \n",
    "    #clustermap\n",
    "\n",
    "    fault_num1 = data.groupby([name_fault, name_fea])[data.columns[0]].count().unstack()\n",
    "\n",
    "    ratio = fault_num1 / fault_num1.sum()  \n",
    "\n",
    "    g1 = sns.clustermap(ratio, \n",
    "                        cmap=plt.get_cmap('RdBu'),\n",
    "                        vmax=1,\n",
    "                        vmin=-1,\n",
    "                        linewidth=0,\n",
    "                        figsize=(10, 10),\n",
    "                        row_cluster=False,\n",
    "                        col_cluster=False\n",
    "                    )\n",
    "    plt.title('fault ratio')\n",
    "\n",
    "#plot\n",
    "#plot_fun('SYNC_ORG_NO', 'FAULT_TYPE')\n",
    "#get_dummies\n",
    "SYNC_ORG_dummies = coo_matrix(pd.get_dummies(data['SYNC_ORG_NO']))\n",
    "\n",
    "#ORG_dummies.drop(['33101', '33407', '33411'], axis=1, inplace=True)\n",
    "\n",
    "#data = data.join(SYNC_ORG_dummies)\n",
    "\n",
    "#data.drop(['SYNC_ORG_NO'], axis=1, inplace=True)\n",
    "#del ORG_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#ORG\n",
    "data['ORG_NO'].describe()\n",
    "#plot\n",
    "\n",
    "#plot\n",
    "#plot_fun('ORG_NO', 'FAULT_TYPE')\n",
    "#get_dummies\n",
    "ORG_freq = data['ORG_NO'].value_counts().index[data['ORG_NO'].value_counts().values<100]\n",
    "data['ORG_NO'] = data['ORG_NO'].replace(ORG_freq.values, 0)\n",
    "ORG_dummies = coo_matrix(pd.get_dummies(data['ORG_NO']))# 转化为稀疏矩阵\n",
    "#ORG_dummies.drop(['33101', '33407', '33411'], axis=1, inplace=True)\n",
    "print ORG_dummies.shape\n",
    "#data = data.join(ORG_dummies)\n",
    "#data.drop(['ORG_NO'], axis=1, inplace=True)\n",
    "#del ORG_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORG故障类型统计\n",
    "- 各个地区的故障数量不同，前8个到95%\n",
    "- 从ORG与FAULT_TYPE统计图可以看出，不同地区的故障类型分布有所不同，所以认为ORG对于FAULT_TYPE类型的识别是有用的。\n",
    "- 故障类型分布图显示了每个地区的故障类型占比\n",
    "- 有几个地区故障类型数据较少[33101,33407,33411]，对于故障类型识别用处不大，删除\n",
    "- 对属性做了二元变换处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPEC_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SPEC_CODE\n",
    "data['SPEC_CODE'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['SPEC_CODE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spec_freq = data['SPEC_CODE'].value_counts().index[data['SPEC_CODE'].value_counts().values<500]\n",
    "#spec_mapping = {label:idx for label,idx in zip(spec_freq, np.zeros(len(spec_freq)))}\n",
    "print spec_freq.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['SPEC_CODE'].value_counts()\n",
    "data['SPEC_CODE'] = data['SPEC_CODE'].replace(spec_freq.values, 0)\n",
    "print data['SPEC_CODE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "#plot_fun('SPEC_CODE', 'FAULT_TYPE')\n",
    "\n",
    "#get_dummies\n",
    "SPEC_dummies = coo_matrix(pd.get_dummies(data['SPEC_CODE']))\n",
    "\n",
    "#SPEC_dummies.drop(['103', '121', '129', '131'], axis=1, inplace=True)\n",
    "\n",
    "#data = data.join(SPEC_dummies)\n",
    "#data.drop(['SPEC_CODE'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPEC_CODE故障类型统计\n",
    "- SPEC_CODE故障类型同样呈现分布不均匀状态\n",
    "- 前两类设备类型数据达到98%\n",
    "- 每种故障类型的SPEC_CODE基本相似\n",
    "- 故障类型分布图显示了每种SPEC_CODE故障类型占比\n",
    "- 删除极少出现的SPEC_CODE故障类型\n",
    "- 对属性进行二元变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANUFACTURER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['MANUFACTURER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spec_freq = data['MANUFACTURER'].value_counts().index[data['MANUFACTURER'].value_counts().values<500]\n",
    "data['MANUFACTURER'] = data['MANUFACTURER'].replace(spec_freq.values, 0)\n",
    "print len(data['MANUFACTURER'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "#plot_fun('MANUFACTURER', 'FAULT_TYPE', fontsize=1)\n",
    "'''\n",
    "#cluster encoding\n",
    "from scipy.cluster.hierarchy import fclusterdata\n",
    "fault_num3 = data.groupby(['FAULT_TYPE', 'MANUFACTURER'])[data.columns[0]].count().unstack()\n",
    "\n",
    "MAN_ratio = fault_num3 / fault_num3.sum()\n",
    "MAN_ratio_T = MAN_ratio.T\n",
    "\n",
    "clusters = fclusterdata(np.array(MAN_ratio_T), 1)\n",
    "clusters_mapping = {label:idx for label,idx in zip(MAN_ratio.columns, clusters)}\n",
    "\n",
    "\n",
    "data['MANUFACTURER'] = data['MANUFACTURER'].map(clusters_mapping)\n",
    "'''\n",
    "#get_dummies\n",
    "MAN_dummies = coo_matrix(pd.get_dummies(data['MANUFACTURER']))\n",
    "#data = data.join(MAN_dummies)\n",
    "#data.drop(['MANUFACTURER'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MANUFACTURER故障类型统计\n",
    "- MANUFACTURER故障类型同样呈现分布不均匀状态,浙江省一共有80家供应商，电表数前30家占90%\n",
    "- 前两类故障类型数据达到98%\n",
    "- 每种故障类型的供应商分布不同\n",
    "- 故障类型分布图显示了每种供应商故障类型占比，应用分层聚类方法将具有相似故障类型分布的供应商进行合并\n",
    "- 对属性进行二元变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# use month distribution\n",
    "c1 = data.groupby(['month']).size()\n",
    "c1.plot(kind='bar', figsize=(12, 6))\n",
    "\n",
    "c2 = data.groupby(['month', 'FAULT_TYPE']).size().unstack().reindex(index=np.arange(data.month.min(), data.month.max()+1)).fillna(0)\n",
    "c2.plot(kind='bar', figsize=(12, 12), subplots=True)\n",
    "\n",
    "c3 = data.groupby(['month', 'SYNC_ORG_NO']).size().unstack().reindex(index=np.arange(data.month.min(), data.month.max()+1)).fillna(0)\n",
    "c3.plot(kind='bar', figsize=(12, 12), subplots=True)\n",
    "'''\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data['month'] = min_max_scaler.fit_transform(data['month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用寿命-故障类型统计\n",
    "- 对浙江省故障电表使用寿命进行了统计，可看出其分布基本为正态分布，符合客观规律。\n",
    "- 使用寿命-故障类型图显示了每种故障类型的使用寿命分布情况，基本为正态分布，但是其分布参数有所不同，可以用来作为分类特征。\n",
    "- 使用寿命-供电所分布图，不同供电所的使用寿命分布有区别，可以得出供电所对电表使用寿命有影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## FAULT_MONTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['FAULT_MONTH'] = pd.Categorical(data['FAULT_MONTH'], ordered=True)\n",
    "\n",
    "#m1 = data.groupby(['FAULT_MONTH', 'FAULT_TYPE']).size().unstack().reindex(index=np.arange(data.FAULT_MONTH.min(), data.FAULT_MONTH.max()+1)).fillna(0)\n",
    "#m1.plot(kind='bar', figsize=(12, 12), subplots=True)\n",
    "#plot_fun('FAULT_MONTH', 'FAULT_TYPE', fontsize=1)\n",
    "'''\n",
    "fault_num4 = data.groupby(['FAULT_TYPE', 'FAULT_MONTH'])[data.columns[0]].count().unstack()\n",
    "\n",
    "FAUMON_ratio = fault_num4 / fault_num4.sum()\n",
    "FAUMON_ratio_T = FAUMON_ratio.T\n",
    "\n",
    "clusters = fclusterdata(np.array(FAUMON_ratio_T), 0.70)\n",
    "clusters = clusters+20\n",
    "print clusters\n",
    "\n",
    "clusters_mapping = {label:idx for label,idx in zip(FAUMON_ratio.columns, clusters)}\n",
    "\n",
    "\n",
    "data['FAULT_MONTH'] = data['FAULT_MONTH'].map(clusters_mapping)\n",
    "'''\n",
    "#get_dummies\n",
    "FAUMON_dummies = coo_matrix(pd.get_dummies(data['FAULT_MONTH']))\n",
    "\n",
    "data['INST_MONTH'] = pd.Categorical(data['INST_MONTH'], ordered=True)\n",
    "INSMON_dummies = coo_matrix(pd.get_dummies(data['INST_MONTH']))\n",
    "#data = data.join(FAUMON_dummies)\n",
    "#data.drop(['FAULT_MONTH'], axis=1, inplace=True)\n",
    "#del fault_num4, FAUMON_ratio, FAUMON_ratio_T, clusters, clusters_mapping, FAUMON_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 故障月份-故障类型统计\n",
    "- 故障月份-故障数量统计表显示了不同月份故障数量的分布，分布不是很均匀\n",
    "- 故障月份-故障类型图显示了每月的故障类型分布情况，每个月的故障类型占比基本相似，是比较弱的分类特征。\n",
    "- 故障月份-故障类型分布图，不同月份故障类型占比基本相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['COMM_MODE'].value_counts()\n",
    "#plot_fun('COMM_MODE', 'FAULT_TYPE')\n",
    "COMM_freq = data['COMM_MODE'].value_counts().index[data['COMM_MODE'].value_counts().values<100]\n",
    "data['COMM_MODE'] = data['COMM_MODE'].replace(COMM_freq.values, 0)\n",
    "COMM_dummies = coo_matrix(pd.get_dummies(data['COMM_MODE']))# 转化为稀疏矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#整合数据\n",
    "data = data.join(pd.DataFrame(bmat([[SYNC_ORG_dummies, ORG_dummies, SPEC_dummies, MAN_dummies, INSMON_dummies, COMM_dummies]]).toarray()))\n",
    "data.drop(['FAULT_MONTH','SYNC_ORG_NO', 'ORG_NO', 'SPEC_CODE', 'MANUFACTURER', 'FAULT_TYPE_1', 'FAULT_DATE1', \n",
    "           'INST_DATE1', 'COMM_MODE', 'INST_MONTH','month'], axis=1, inplace=True)\n",
    "\n",
    "del SYNC_ORG_dummies, ORG_dummies, SPEC_dummies, MAN_dummies, FAUMON_dummies, INSMON_dummies, COMM_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习算法故障预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from scipy.sparse import coo_matrix\n",
    "data_X = data\n",
    "data_y = data['FAULT_TYPE']\n",
    "#encode label\n",
    "le = preprocessing.LabelEncoder()\n",
    "data_y = le.fit_transform(data_y)\n",
    "data['FAULT_TYPE'] = data_y\n",
    "'''\n",
    "data_X1 = csc_matrix(data_X.ix[:200000]) \n",
    "data_X2 = csc_matrix(data_X.ix[200001:400000])\n",
    "data_X3 = csc_matrix(data_X.ix[400001:])\n",
    "data_X4 = bmat([[data_X1], [data_X2], [data_X3]], format='coo')\n",
    "del data_X1, data_X2, data_X3\n",
    "'''\n",
    "train, test= train_test_split(data_X, test_size=0.4, random_state=27, stratify=data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e35d9914ea9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/.local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m             return _methods._sum(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 1844\u001b[0;31m                                 out=out, **kwargs)\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/.local/lib/python2.7/site-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.enable()\n",
    "gc.collect()\n",
    "#将测试中出现的训练数据删除\n",
    "l = []\n",
    "train = coo_matrix(train)\n",
    "test = coo_matrix(test)\n",
    "gc.collect()\n",
    "for n in range(test.shape[0]):\n",
    "    if np.sum(np.where(test.getrow(n) == train.toarray())):\n",
    "        l.append(n)\n",
    "       \n",
    "    if (n%50000)==0:\n",
    "        print n\n",
    "print test.shape\n",
    "test = pd.DataFrame(test.toarray())\n",
    "test.drop(l, inplace=True)\n",
    "print test.shape\n",
    "train_y = train.getcol(-1)\n",
    "test_y = test.getcol(-1)\n",
    "#train.drop('FAULT_TYPE', axis=1, replace=True)\n",
    "#test.drop('FAULT_TYPE', axis=1, replace=True)\n",
    "train = coo_matrix(train.toarray()[:,:-1])\n",
    "test = coo_matrix(test.toarray()[:,:-1])\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "TRAIN = True  #是否训练\n",
    "CV = False\n",
    "#split train set and test set\n",
    "dtrain = xgb.DMatrix(train, train_y)\n",
    "dtest = xgb.DMatrix(test)\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    learning_rate = 0.2,\n",
    "    n_estimators = 660,\n",
    "    max_depth = 8,\n",
    "    colsample_bytree = 0.8,\n",
    "    subsample = 0.9,\n",
    "    objective = 'multi:softmax',\n",
    "    min_child_weight = 1,\n",
    "    gamma = 2,\n",
    "    seed = 27\n",
    "    )\n",
    "\n",
    "param = clf.get_xgb_params()\n",
    "param['num_class'] = 11\n",
    "if CV:\n",
    "    cvresult = xgb.cv(param, dtrain, num_boost_round=2000, nfold=3, stratified=True,\n",
    "                  metrics='merror', early_stopping_rounds=10,verbose_eval=True)\n",
    "    clf.set_params(n_estimators=cvresult.shape[0])   #set n_estimators as cv rounds\n",
    "if TRAIN:\n",
    "    clf.fit(train, train_y, eval_metric='merror')\n",
    "else:\n",
    "    clf = pickle.load(open(\"zhejiang_4_all.pkl\", \"rb\"))\n",
    "        \n",
    "\n",
    "ypred_xgb = clf.predict(test)\n",
    "ypred_xgb = le.inverse_transform(ypred_xgb)\n",
    "test_y_xgb = le.inverse_transform(test_y)\n",
    "#print model report:\n",
    "print(classification_report(test_y_xgb, ypred_xgb))\n",
    "print(confusion_matrix(test_y_xgb, ypred_xgb))\n",
    "\n",
    "xgb.plot_importance(clf.booster())\n",
    "pickle.dump(clf, open(\"zhejiang_4_all.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 召回率(Recall)=  系统检索到的相关文件 / 系统所有相关的文件总数\n",
    "* 准确率(Precision) =  系统检索到的相关文件 / 系统所有检索到的文件总数\n",
    "* f1 = 2*Recall*Precision / (Recall+Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    }
   ],
   "source": [
    "param_test1 = {'max_depth':range(5,12,2), 'min_child_weight':range(1,7,2)}\n",
    "gsearch1 = GridSearchCV(estimator=clf, param_grid = param_test1, scoring='accuracy',n_jobs=-1,cv=2, verbose=True)\n",
    "gsearch1.fit(train, train_y)\n",
    "print gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['FAULT_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SGDClassifier\n",
    "使用随机梯度下降线性分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于线性不可分情况，使用rbf核将数据映射到高维空间中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler, Nystroem\n",
    "\n",
    "USE_RBF = False   #True：RBFSampler, False:Nystroem\n",
    "if USE_RBF:\n",
    "    rbf_feature = RBFSampler(gamma=1, random_state=1)\n",
    "    train_SGD = rbf_feature.fit_transform(train)\n",
    "    test_SGD = rbf_feature.transform(test)\n",
    "else:\n",
    "    Nys_feature = Nystroem(gamma=1, random_state=1)\n",
    "    train_SGD = Nys_feature.fit_transform(train)\n",
    "    test_SGD = Nys_feature.transform(test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "USE_GridSearch = False\n",
    "clf = SGDClassifier(loss='modified_huber', alpha=0.01, n_iter=100, class_weight=\"balanced\", random_state=27)\n",
    "if USE_GridSearch:\n",
    "    param_test1 = {'loss':['hinge', 'log','modified_huber', 'squared_hinge', 'perceptron'], 'alpha':[0.1, 0.01, 0.01, 0.0001]}\n",
    "    gsearch1 = GridSearchCV(estimator=clf, param_grid = param_test1, scoring='accuracy', n_jobs=-1,cv=2, verbose=True)\n",
    "    gsearch1.fit(train_SGD, train_y)\n",
    "    print gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
    "    clf = gsearch1\n",
    "else:\n",
    "    clf.fit(train_SGD, train_y)\n",
    "ypred_sgd = clf.predict(test_SGD)\n",
    "ypred_sgd = le.inverse_transform(ypred_sgd)\n",
    "test_y_sgd = le.inverse_transform(test_y)\n",
    "#print model report:\n",
    "print(classification_report(test_y_sgd, ypred_sgd))\n",
    "print(confusion_matrix(test_y_sgd, ypred_sgd))\n",
    "pickle.dump(clf, open(\"zhejiang_4_SGD.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "USE_GridSearch = False\n",
    "n_neighbors = 50\n",
    " \n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
    "if USE_GridSearch:\n",
    "    param_test1 = {'n_neighbors':range(20,60,10), 'weights':['uniform', 'distance']}\n",
    "    gsearch1 = GridSearchCV(estimator=clf, param_grid = param_test1, scoring='accuracy', n_jobs=-1,cv=2, verbose=True)\n",
    "    gsearch1.fit(train, train_y)\n",
    "    print gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
    "    clf = gsearch1\n",
    "else:\n",
    "    clf.fit(train, train_y)\n",
    "ypred_knn = clf.predict(test)\n",
    "ypred_knn = le.inverse_transform(ypred_knn)\n",
    "test_y_knn = le.inverse_transform(test_y)\n",
    "#print model report:\n",
    "print(classification_report(test_y_knn, ypred_knn))\n",
    "print(confusion_matrix(test_y_knn, ypred_knn))\n",
    "pickle.dump(clf, open(\"zhejiang_4_KNN.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* XGBoost算法使用决策树作为弱分类器，如果训练数据可分的情况下将会一直拟合数据知道训练准确率100%，可以证明数据不可分\n",
    "* SGDClassifier是针对数据量较大的线性分类器，当线性分类无效时，使用rbf将数据映射到高维空间中，再采用线性分类其进行分类，效果不佳\n",
    "* 使用KNN算法\n",
    "## 可能存在的问题\n",
    "1. 是否可以将使用时长、故障月份作为故障类型的预测属性\n",
    "* 在真实测试中，在安装电表时，电表还未投入使用无法得到使用时长、故障月份数据。\n",
    "2. 测试集如何划分才能有效的评测分类器性能好坏\n",
    "* 划分测试集的目的是使训练与测试集中的数据不同，以测试训练所得的分类器在未知数据上的泛化能力，如果有很大部分是相同的话划分训练测试的初衷是什么（超过一半）。\n",
    "* 由于相同属性会有不同类别的频次，这个信息可以被树类等机器学习算法学到，所以相同数据的重复数据在没有更加有区分度的属性之前暂时保留。\n",
    "* 划分测试与训练的基本原则是保持其同分布，由于测试数据从所有数据中抽取30%，是为了代表这类问题的普遍性。使用树类的决策树算法，可以无限拟合训练数据，对于这类的数据很可能机器只是记忆了数据，一个模型想要训练准确率高很容易，但是往往一个越简单的算法模型我们认为他具有更好的泛化能力。训练的数据可能在真实的情况中再出现，想要看模型对于训练数据的性能好坏，观察training曲线就可以，没有必要再加入到测试集中，一个好的分类器应该是在这类问题上的任何情况都能表现的很好，而把训练数据加入到测试中，就像把一道测试题的结果已经提前‘泄露’给了训练模型。我们要实现的是对与故障预测这一类问题的分类器，我们要从有限的历史数据中学习出一个泛化能力较好的分类器，训练数据只是大量历史与未来数据中的一部分，这个分类器在这个数据上表现怎么样并不能代表这类问题就能解决好，往往是他有了能够对未知数据较好的处理能力，就像学习一类题目，能举一反三，才能说这类问题我们解决了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
